# Real-Time-Collision-Risk-Estimation-via-Sensor-Fusion
  This project implements an offline embedded collision risk estimation system that fuses camera-based motion estimation with distance sensing under strict hardware and network constraints. The system is designed for environments where connectivity, external computation, or GPS is unavailable, requiring all sensing and processing to occur locally and deterministically.

  The primary goal of the project is to study how multimodal sensor fusion improves collision risk estimation compared to single-sensor approaches on resource-constrained platforms. A monocular camera is used to extract relative motion information through computationally efficient optical flow methods, while a distance sensor provides absolute ranging data with low latency. These signals are time-aligned and fused to estimate relative velocity and time-to-collision in real time.

  The system architecture emphasizes predictability, modularity, and interpretability. Algorithms rely on classical computer vision and signal processing techniques rather than data-driven models, enabling precise control over latency and computational load. Each sensing and processing module is independently testable, allowing systematic characterization of sensor noise, timing error, and failure modes.

  Experimental evaluation was conducted across varying distances, motion profiles, and lighting conditions. Performance metrics including frame rate, end-to-end latency, ranging error, and collision prediction lead time were logged and analyzed. Results indicate that fusing visual motion cues with distance measurements increases robustness, particularly in scenarios where individual sensing modalities degrade due to noise or environmental conditions.

  This project emphasizes core computer engineering principles, including embedded system design, real-time processing, hardwareâ€“software integration, and empirical system evaluation. More broadly, it explores fundamental challenges in perception for safety-critical systems operating under realistic constraints and establishes a foundation for future extensions involving closed-loop control, additional sensing modalities, or formal verification methods.
